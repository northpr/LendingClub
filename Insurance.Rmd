
author: "Pat Patawee"
date: "7/2/2021"
output: html_document


# Lending Clubs and Loan, Project Overview 
This data uses publicly avaliable data from the LendingClub, there are 9,578 observarions, each representing a 3-year loan that was funded through the LendingClub platform between May, 2007 to February, 2010. We will try to predict **fico** and **not.fully.paid** by using other variables as independent variables

Language in this notebook : R

In this work I will included
- Cleaning
- Virtualization
- Create the model (Linear Regression, Logistic Regression, Decision Tree, Random Forest)
- Clustering to seperate the group of customer

## Setting up environment
Import important libraries
```{r}

library(dplyr)
library(ggplot2)
library(caTools)
library(ROCR)
library(sqldf)
library(corrplot)
library(vcd)
library(Metrics)
library(rpart.plot)
library(rpart)
library(fastDummies)
library(randomForest)
library(pROC)
library(kableExtra)

```

Some of the variables are missing and changing variales type.

```{r}
rawdf = read.csv("loans.csv")
df = rawdf
df = df[c(-1)]
```

Loading the data and delete the row number columns because it doesn't matter if we use it or not

```{r}
str(df)
head(df)
```
## Cleaning the data
Change char type to num
```{r}
cols_int = c(1,12,13)
df[cols_int] = sapply(df[cols_int],as.integer)
cols_num = c(6,10)
df[cols_num] = sapply(df[cols_num],as.numeric)

```

Change purpose to factor type because we couldn't know what type of purpose is greater than others. Then we should change to the factor type.
```{r}
df$purpose <-factor(df$purpose)
str(df)
```

```{r}
sum(is.na(df))
(sum(is.na(df))/nrow(df)) * 100

```
number of missing data and percentage

We can delete all the rows which has the missing value because it's only 1.9% pf the dataset

```{r}
df = na.omit(df)
nrow(df)
sum(is.na(df))
```

Delete the row with missing values and we still have 9508 rows

```{r}
summary(df)
```





<!-- Move the purpose to 2nd column and change type to numeric -->
<!-- ```{r} -->
<!-- tmpdata = tmpdata[c(-2)] -->
<!-- tmpdata = tmpdata %>% -->
<!--   relocate(purpose,.after =credit.policy) -->
<!-- tmpdata[2] = sapply(tmpdata[2],as.numeric) -->

<!-- str(tmpdata) -->
<!-- df = tmpdata -->
<!-- ``` -->

After basic summary to the data we can feel that some variables such as int.rate, fico, days.with.cr.line, revol.bal, revol.util, inq.last.6mths and delinq.2yrs should be focus on. Due to the maximum of the data is not seems to be correct.


Find the correlation of the data to fico
```{r}
cor_matrix = cor(df[-c(2)], method = "spearman")
names = rownames(cor_matrix)
abs_cor = abs(cor_matrix)
data = data.frame(X_var = names,abs_cor = abs_cor,cor = cor_matrix)

cortmp = data[order(data$abs_cor.fico)]
cortmp['abs_cor.fico']


```
We could know that 'int.rate','revol.util', and 'credit.policy' has correlation with 'fico'

```{r}
cor_matrix = cor(df[-c(2)], method = "spearman",)
cor_matrix = round(cor_matrix, 2)
cor_matrix
corrplot(cor_matrix,method='number')

```

Plotting the correlation map for better understanding.
 - The correlation between fico and int.rate is inverse so it means that higher the value of fico are the lower of interest rate.
 - Fico and revol.util (-0.52)
 - and others are also have high correlation between 2 variables

## Check for the outliers

From what I've said before, some of the variables need to be check carefully because it's outliers and can break our model

```{r}

bar <- ggplot(df,aes(fico))+geom_histogram(aes(fill=factor(not.fully.paid)),color='black',bins = 40,alpha=0.5)
bar+scale_fill_manual(values = c("#FF5733","#44FF33"))+theme_bw()

boxplot(fico~purpose,data=df,col='orange')
```



```{r}
ggplot(df) +
  aes(x = fico, y = int.rate) +
  geom_point(shape = "circle", size = 1.5, colour = "#228B22")
```

We have outliers on **fico** and **int.rate** as you can see from the graph. It's far beyond what we can imagine. we need to remove those specific int.rate and fico

```{r}
df = df[-which(df$fico > 850),]
df = df[-which(df$int.rate> 5),]
df = df[-which(df$revol.util > 500),]
df = df[-which(df$revol.bal > 75000),]


```

```{r}
ggplot(df) +
  aes(x = fico, y = int.rate) +
  geom_point(shape = "circle", size = 1.5, colour = "#228B22")

ggplot(df) +
  aes(x = revol.bal, y = revol.util) +
  geom_point(shape = "circle", size = 1.5, colour = "#228B22")

ggplot(df) +
  aes(x = fico, y = inq.last.6mths) +
  geom_point(shape = "circle", size = 1.5, colour = "#228B22")

ggplot(df) +
  aes(x = fico, y = delinq.2yrs) +
  geom_point(shape = "circle", size = 1.5, colour = "#228B22")
```

```{r}
bar<- ggplot(df,aes(factor(purpose)))+geom_bar(aes(fill=factor(not.fully.paid)),position='dodge')
bar+theme(axis.text.x =element_text(angle = 90,size = 10,vjust = 0.5))+theme_bw()

box <- ggplot(df,aes(fico))+geom_histogram(aes(fill=factor(not.fully.paid)),color='black',bins = 40,alpha=0.5)
box+scale_fill_manual(values = c("#FF5733","#44FF33"))+theme_bw()

boxplot(fico~purpose,data=df,col='orange')
```

and now we delete the outliers of variable int.rate, revol.util and fico

# Create A Model - Supervised Learning
### Linear Model
To predict the fico score from other variables

Now we need to split before create linear regression model. I normally using split ratio at 0.7

```{r}
set.seed(99)
split = sample.split (df$fico, SplitRatio = 0.70)
df_train = subset(df, split == TRUE)
df_test = subset(df, split == FALSE)
```

# 3
Building Linear Model
```{r}
linearMod = lm(fico ~ .,data = df_train)
summary(linearMod)
df_test$FicoLinear = predict(linearMod,df_test)
```

```{r}
LinearFiRM = rmse(df_test$fico, df_test$FicoLinear)
LinearFiRM
cat("The RMSE of Linear Regression model is", round(LinearFiRM,2))
```
Now we have rmse value

```{r}
linearMod = lm(fico ~ int.rate * purpose+ int.rate*installment + dti*days.with.cr.line +delinq.2yrs*pub.rec*not.fully.paid,data = df_train)
summary(linearMod)
df_test$FicoLinear = predict(linearMod,df_test)
```

```{r}
LinearFiRM = rmse(df_test$fico, df_test$FicoLinear)
LinearFiRM
```
### Logistic Regression
split at the rate of 0.7
```{r}
set.seed(99)
split = sample.split (df$not.fully.paid, SplitRatio = 0.70)
df_train = subset(df, split == TRUE)
df_test = subset(df, split == FALSE)
```

For Baseline Accuracy

```{r}
t = table(df_test$not.fully.paid)
t

accuracy = t[1]/sum(t)
cat("The accuracy is", round(accuracy,2))
```


```{r}
LogRegModel = glm(not.fully.paid ~., family = binomial, df_train) 
summary(LogRegModel)
```

```{r}
df_test$PredictedRisk = predict(LogRegModel, type = "response", df_test)
LogRegPredict = predict(LogRegModel, type = "response", df_test)
plot(df_test$PredictedRisk)
```

```{r}
df_test$PredictedRisk_Cat = ifelse(df_test$PredictedRisk > 0.25,1,0)
t = table(df_test$not.fully.paid, df_test$PredictedRisk_Cat)
```

Find the accuracy of the model by using confusion matrix
```{r}
t
accuracy = sum(diag(t)/sum(t))
cat("The accuracy is", round(accuracy,2))
```

Find the area under the curve (AUC)
```{r}
ROCRpred = prediction (LogRegPredict, df_test$not.fully.paid)
ROCRperf = performance (ROCRpred, "tpr", "fpr")
plot (ROCRperf, colorize = TRUE, print.cutoffs.at = seq (0, 1, by = 0.01), text.adj = c(-0.2, 1.7))
abline(v=0.25)
AUCVal6 = as.numeric (performance (ROCRpred, "auc") @y.values) 

cat("at threshold = 0.25 AUC Value is :", AUCVal6)

```


I want to leep the false positve at around 25% or 0.25 so I need to tune the threshold value to mazimize true positive rate

```{r}
LogRegModel = glm(not.fully.paid ~ ., family = binomial, df_train) 
summary(LogRegModel)
```

```{r}
df_test$PredictedRisk = predict(LogRegModel, type = "response", df_test)
LogRegPredict = predict(LogRegModel, type = "response", df_test)

plot(df_test$PredictedRisk)
```

Create the confusion matrix to calculate the accuracy of the model

```{r}
df_test$PredictedRisk_Cat = ifelse(df_test$PredictedRisk > 0.185,1,0)
t = table(df_test$not.fully.paid, df_test$PredictedRisk_Cat)

t
```

Calculate the accuracy of the model
```{r}
accuracy = sum(diag(t)/sum(t))
cat("The accuracy is", round(accuracy,2))
```

Area under curve
```{r}
ROCRpred = prediction (LogRegPredict, df_test$not.fully.paid)
ROCRperf = performance (ROCRpred, "tpr", "fpr")
plot (ROCRperf, colorize = TRUE, print.cutoffs.at = seq (0, 1, by = 0.1), text.adj = c(-0.2, 1.7))

as.numeric (performance (ROCRpred, "auc") @y.values) 

```

#### I want to create simpler model
Using only int.rate to create Regression model
```{r}
LogRegModel = glm(not.fully.paid ~ int.rate , family = binomial, df_train) 
summary(LogRegModel)
```
Predict the risk by using only interest rate
```{r}

df_test$PredictedRisk = predict(LogRegModel, type = "response", df_test)
LogRegPredict = predict(LogRegModel, type = "response", df_test)


plot(df_test$PredictedRisk)
df_test$PredictedRisk_Cat = ifelse(df_test$PredictedRisk > 0.20,1,0)
t = table(df_test$not.fully.paid, df_test$PredictedRisk_Cat)

```
Accuracy of my simple model
```{r}
t
accuracy = sum(diag(t)/sum(t))
cat("The accuracy of the simple model is", round(accuracy,2))

```


```{r}
ROCRpred = prediction (LogRegPredict, df_test$not.fully.paid)
ROCRperf = performance (ROCRpred, "tpr", "fpr")
plot (ROCRperf, colorize = TRUE, print.cutoffs.at = seq (0, 1, by = 0.05), text.adj = c(-0.2, 1.7))

AUCVal = as.numeric (performance (ROCRpred, "auc") @y.values) 

cat("The AUC Value of my model is", round(AUCVal,3))

```

# 12
### Decision Tree

```{r}
set.seed(99)
split = sample.split (df$fico, SplitRatio = 0.70)
df_train = subset(df, split == TRUE)
df_test = subset(df, split == FALSE)
```

Decision Tree with Regression predict fico
```{r}
prpDCTR = rpart(fico ~ ., df_train,control = rpart.control(cp= 0.004))
PredictFico = predict(prpDCTR, df_test, method = "anova")
plot(df_test$fico, PredictFico)
```
How the trees are
```{r}
DeciTrRM=sqrt(mean((df_test$fico - PredictFico)^2))
rpart.plot(prpDCTR)
rpart.rules(prpDCTR, cover=TRUE)
plotcp(prpDCTR)

```
Find the diffrent to compare between Linear Regression and Decision Tree
```{r}

cat("Different of RMSE is Decision Tree Model RMSE - Linear Model RMSE: ",DeciTrRM - LinearFiRM)
```




#### Decision Tree with Classification predict **not.fully.paid**
Split the data for train and test
```{r}
set.seed(99)
split = sample.split (df$not.fully.paid, SplitRatio = 0.70)
df_train = subset(df, split == TRUE)
df_test = subset(df, split == FALSE)
```

```{r}
nfpDCTM = rpart(not.fully.paid~.,data=df_train, method = 'class',cp=0.002)
prp(nfpDCTM)
rpart.plot(nfpDCTM)
printcp(nfpDCTM)
plotcp(nfpDCTM)
```
(accuracy,3))
# ```

Confusion Matrix for Decision Tree Model
```{r}
nfpPred = predict(nfpDCTM, newdata=df_test,type='prob')
nfpPred_Cat = ifelse(nfpPred[, 2] > 0.2, 1, 0)

table = table(nfpPred_Cat, df_test$not.fully.paid)
table

accuracy = sum(diag(table))/(sum(table))
cat("The accuracy is", round(accuracy,2))
```

```{r}
ROCRpred = prediction (nfpPred[, 2], df_test$not.fully.paid)
AUCVal12 = as.numeric (performance (ROCRpred, "auc") @y.values) # higher auc value is better
performance (ROCRpred, "tpr", "fpr")
plot (ROCRperf, colorize = TRUE, print.cutoffs.at = seq (0, 1, by = 0.1), text.adj = c(-0.2, 1.7))
cat("The AUC Value of my model is", round(AUCVal12,3))
```

#### Summary of the Decision Tree Model
The area under curve or the accuracy might not be good as logistic regression might it might easier to explain to the customer or the investor.
```{r}
cat('Area under curve of logistic regression:',AUCVal6,'and area under curve of decision tree:',AUCVal12)

cat('

The difference of the area under curve between logistic regression and decision tree:',AUCVal6 - AUCVal12)
```




### Random Forest
Random Forest with Regression to predict fico.
```{r}
set.seed(99)
split = sample.split (df$fico, SplitRatio = 0.70)
df_train = subset(df, split == TRUE)
df_test = subset(df, split == FALSE)
```

```{r}
FicoForest = randomForest(fico~.,df_train, ntree=700, mtry=2)
PredictForest = predict(FicoForest, df_test)
```
Calculate the RMSE
```{r}
RanForRM = round(sqrt(mean((df_test$fico - PredictForest)^2)),2)

cat('RMSE of Random Forest Model:',RanForRM,'and RMSE of Linear Regression Model:',LinearFiRM)


cat("
Different of RMSE is Random Forest Model RMSE - Linear Model RMSE: ",RanForRM - LinearFiRM)
```
Random Forest Model perform slightly better than Linear Regression Model

#### Random Forest with Classification to predict **not.fully.paid**
```{r}
set.seed(99)
split = sample.split (df$not.fully.paid, SplitRatio = 0.70)
df_train = subset(df, split == TRUE)
df_test = subset(df, split == FALSE)
```

```{r}
NFPForest = randomForest(not.fully.paid ~ ., data = df_train, mtry = 3, ntree = 50)
PredictNFP = predict(NFPForest, df_test, type = "class")
```

```{r}
df_test$RandomForest = predict(NFPForest, df_test, type = "class")
plot(PredictNFP)
df_test$PredictedRisk = ifelse(df_test$RandomForest > 0.25,1,0)
table(df_test$not.fully.paid, df_test$PredictedRisk)
```

```{r}
ROCRpred = prediction (PredictNFP, df_test$not.fully.paid)
ROCRperf = performance (ROCRpred, "tpr", "fpr")
plot (ROCRperf, colorize = TRUE, print.cutoffs.at = seq (0, 1, by = 0.1), text.adj = c(-0.2, 1.7))


AUCVal = as.numeric (performance (ROCRpred, "auc") @y.values) 

cat("The AUC Value of my model is", round(AUCVal,3))

```



# Clustering
### Preparing Data for clustering
```{r}
df = rawdf
df = na.omit(df)
df = df[-which(df$fico > 850 ),]
df = df[-which(df$int.rate> 5),]
df = df[-which(df$revol.util > 500),]
df = df[-which(df$revol.bal > 75000),]
df = df[-c(1)]

cols_num = c(1,12,13,6,10)
df[cols_num] = sapply(df[cols_num],as.numeric)



df_cluster = df
df_cluster = fastDummies::dummy_cols(df_cluster, select_columns = "purpose")
df_cluster = df_cluster[-c(2)]




```

Scale and build cluster
```{r}
df_cluster = df_cluster[c(1,2,6,7,9)]
df_cluster = na.omit(df_cluster)
df_scale = scale(df_cluster)
km.out = kmeans(df_scale, 3 ,nstart=20)
```

The suitable number of cluster in this datasets by using Elbow Method

```{r}
N = 10
information = rep ( NA, N )
for ( i in 1: N ){
  KM = kmeans ( df_scale, centers = i, iter.max = 35, nstart = 10 )
  information [ i ] = KM$tot.withinss
}

plot ( information ~ seq ( 1:N ), type = "b", pch = 1, col = 2, ylab = "Total within Sum of Squares", lwd=2,
       xlab = "Number of Clusters", main = "Selecting K by elbow method" )
```
As we can see from the plot we could seperate the type of customer into 3 groups. Or clearly seperate at 2 groups
```{r}
plot(df_cluster, col = km.out$cluster)
```
Seperate the cluster into 3 groups

```{r}
cluster1 = subset(df_cluster, km.out$cluster == 1)
cluster2 = subset(df_cluster, km.out$cluster == 2)
cluster3 = subset(df_cluster, km.out$cluster == 3)

```

# Final
This is all I've done with this project, you might interest to create a column to invest from the loaner and calculate the chance to making profit for more..
